{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 5: K-NNs and Ensemble Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this assignment, answer directly on this Jupyter notebook. Once you're done, please submit the assignment as \"Name_Surname_Assignment5.ipynb\"\n",
    "\n",
    "*Don't forget that commenting your code is very important!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "###  K-NN Implementation\n",
    "\n",
    "- Implement the K-NN algorithm by hand (ie. Don't use the sklearn implementation).\n",
    "- Evaluate and plot model performance for different values of k.\n",
    "\n",
    "For this question, we will be using the classic Iris dataset, available in sklearn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import packages\n",
    "\n",
    "###### Importing packages and knowing what packages you need for a project is crucial. We will not be reminding you which packages you need for each question and for the assignment in general. Please import the packages at your own discretion. Although it is common practice to import all packages at once at the beginning, don't hesitate to revisit the next cell, and import more packages as you may need. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement kNN by hand. It might be useful to store all distances in one array/list\n",
    "\n",
    "### YOUR CODE HERE\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import operator\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Load the Iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
       "0                5.1               3.5                1.4               0.2   \n",
       "1                4.9               3.0                1.4               0.2   \n",
       "2                4.7               3.2                1.3               0.2   \n",
       "3                4.6               3.1                1.5               0.2   \n",
       "4                5.0               3.6                1.4               0.2   \n",
       "\n",
       "   target  \n",
       "0     0.0  \n",
       "1     0.0  \n",
       "2     0.0  \n",
       "3     0.0  \n",
       "4     0.0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loading dataset\n",
    "iris = load_iris()\n",
    "iris_df = pd.DataFrame(data= np.c_[iris['data'], iris['target']],\n",
    "                     columns= iris['feature_names'] + ['target'])\n",
    "\n",
    "# Preview dataset\n",
    "iris_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### In order to evaluate the performance of our kNN implementation, we first split the dataset into training and test sets. A 70/30 split or something similar should suffice. Remember class balance within both train/test sets is important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[6.5, 3. , 5.5, 1.8, 2. ],\n",
       "       [5. , 3.4, 1.6, 0.4, 0. ],\n",
       "       [5.4, 3.9, 1.7, 0.4, 0. ],\n",
       "       [5.8, 2.8, 5.1, 2.4, 2. ],\n",
       "       [4.8, 3. , 1.4, 0.1, 0. ],\n",
       "       [6.6, 2.9, 4.6, 1.3, 1. ],\n",
       "       [5. , 3.2, 1.2, 0.2, 0. ],\n",
       "       [5.7, 2.5, 5. , 2. , 2. ],\n",
       "       [5.7, 2.8, 4.5, 1.3, 1. ],\n",
       "       [6.4, 2.8, 5.6, 2.2, 2. ],\n",
       "       [4.8, 3.1, 1.6, 0.2, 0. ],\n",
       "       [5.8, 2.7, 4.1, 1. , 1. ],\n",
       "       [6.5, 3. , 5.2, 2. , 2. ],\n",
       "       [6.3, 2.7, 4.9, 1.8, 2. ],\n",
       "       [7.3, 2.9, 6.3, 1.8, 2. ],\n",
       "       [5.7, 2.6, 3.5, 1. , 1. ],\n",
       "       [6.1, 3. , 4.9, 1.8, 2. ],\n",
       "       [5. , 3.3, 1.4, 0.2, 0. ],\n",
       "       [6. , 2.2, 5. , 1.5, 2. ],\n",
       "       [5.5, 2.4, 3.7, 1. , 1. ],\n",
       "       [5.5, 2.3, 4. , 1.3, 1. ],\n",
       "       [4.8, 3.4, 1.9, 0.2, 0. ],\n",
       "       [5.1, 3.5, 1.4, 0.3, 0. ],\n",
       "       [6.7, 3.3, 5.7, 2.1, 2. ],\n",
       "       [5.8, 2.7, 3.9, 1.2, 1. ],\n",
       "       [7.1, 3. , 5.9, 2.1, 2. ],\n",
       "       [5.1, 3.7, 1.5, 0.4, 0. ],\n",
       "       [7.4, 2.8, 6.1, 1.9, 2. ],\n",
       "       [5.7, 2.8, 4.1, 1.3, 1. ],\n",
       "       [5.7, 2.9, 4.2, 1.3, 1. ],\n",
       "       [6.6, 3. , 4.4, 1.4, 1. ],\n",
       "       [6.4, 3.2, 4.5, 1.5, 1. ],\n",
       "       [5.6, 3. , 4.1, 1.3, 1. ],\n",
       "       [4.7, 3.2, 1.6, 0.2, 0. ],\n",
       "       [7.9, 3.8, 6.4, 2. , 2. ],\n",
       "       [5.4, 3. , 4.5, 1.5, 1. ],\n",
       "       [7.7, 3. , 6.1, 2.3, 2. ],\n",
       "       [4.9, 3. , 1.4, 0.2, 0. ],\n",
       "       [6.5, 3. , 5.8, 2.2, 2. ],\n",
       "       [4.9, 2.5, 4.5, 1.7, 2. ],\n",
       "       [5.7, 4.4, 1.5, 0.4, 0. ],\n",
       "       [4.8, 3.4, 1.6, 0.2, 0. ],\n",
       "       [5.5, 2.5, 4. , 1.3, 1. ],\n",
       "       [6. , 2.7, 5.1, 1.6, 1. ],\n",
       "       [5.1, 3.8, 1.5, 0.3, 0. ]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### YOUR CODE HERE - Shuffle dataset, then split for balanced classes. \n",
    "\n",
    "iris_shuffled = shuffle(iris_df)\n",
    "iris_shuffled.head(10)\n",
    "iris_shuffled.reset_index(inplace=True, drop=True)\n",
    "\n",
    "trainSplit = 0.7\n",
    "pivot = int(trainSplit * iris_shuffled.shape[0])\n",
    "\n",
    "iris_train = iris_shuffled[:pivot].copy() # Separating the features from target variables is not essential here\n",
    "iris_test = iris_shuffled[pivot:].copy()\n",
    "\n",
    "print(type(iris_test))\n",
    "iris_train = iris_train.values.tolist()\n",
    "iris_test = iris_test.values.tolist()\n",
    "\n",
    "iris_train = np.asarray(iris_train)\n",
    "iris_test = np.asarray(iris_test)\n",
    "\n",
    "iris_train\n",
    "iris_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Defining a distance metric\n",
    "\n",
    "###### To define similarity between two given points, we must define a distance metric. Write a method that takes  two points as input and returns the distance between the points. Look at the format of each point in the sample case below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE - Write method that returns Euclidean distance between two points\n",
    "\n",
    "def getDistance(p1, p2):\n",
    "    \"\"\" Calculates the Euclidean distance between two points. Returns dist float\"\"\"\n",
    "    length = len(p1) - 1\n",
    "    dist = 0\n",
    "    \n",
    "    for i in range(length):\n",
    "        dist += pow((p1[i] - p2[i]), 2)\n",
    "    return math.sqrt(dist)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######  Let's test our newly written method on the following samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.4641016151377544\n"
     ]
    }
   ],
   "source": [
    "data1 = [2, 2, 2, 'a']\n",
    "data2 = [4, 4, 4, 'b']\n",
    "distance = getDistance(data1, data2)\n",
    "\n",
    "print(distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Finding k nearest neighbours\n",
    "\n",
    "###### Now that we've defined a distance metric, we can use it collect the k most similar instances for a new test instance. Write a method calculating the distance between a test point and all training instances, selecting a subset with the smallest distance values. It might be useful to store all the distances in a list/array, since Python has a built-in sorting function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE - Write method calculating the distance for all instances, \n",
    "### selecting a subset with the smallest distance values. \n",
    "\n",
    "def getNeighbours(trainingSet, sample, k):\n",
    "    \"\"\" Calculates k nearest neighbours using a distance metric. Returns neighbours list\"\"\"\n",
    "    distances = []\n",
    "    #length = len(sample) - 1\n",
    "    \n",
    "    for x in range(len(trainingSet)):\n",
    "        dist = getDistance(sample, trainingSet[x])\n",
    "        \n",
    "        distances.append((trainingSet[x], dist))  # Store distances in list \n",
    "        \n",
    "    distances.sort(key=operator.itemgetter(1))\n",
    "    \n",
    "    ### Built in sorting functions in Python ***ONE-LINE get max index***\n",
    "    neighbors = []\n",
    "    \n",
    "    for x in range(k):\n",
    "        neighbors.append(distances[x][0])\n",
    "    return neighbors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######  Let's test our newly written method on the following samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4, 4, 4, 'b'], [3, 3, 3, 'c'], [2, 3, 4, 'd']]\n"
     ]
    }
   ],
   "source": [
    "trainSet = [[2, 2, 2, 'a'], [4, 4, 4, 'b'], [3, 3, 3, 'c'], [2, 3, 4, 'd'], [10,20,30,'e']]\n",
    "#print(len(trainSet))\n",
    "testInstance = [5, 5, 5] \n",
    "\n",
    "neighbors = getNeighbours(trainSet, testInstance, 3)\n",
    "print(neighbors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Now to build a prediction model, write a method that returns a prediction given k nearest neighbours from the previous method. (Hint: one way you can do this is to build a dictionary, and sort the key-value pairs to determine which class occurs the most often!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE - Write method that takes in k nearest neighbours as input, and votes based on the majority class.\n",
    "\n",
    "def predict(neighbours):\n",
    "    \"\"\" Returns predicted class response based off majority vote from k neighbours set\"\"\"\n",
    "    \n",
    "    votes = {}            #create an empty dict*\n",
    "    for i in range(len(neighbours)):\n",
    "        current = neighbours[i][-1] # last val (label in this case)\n",
    "    \n",
    "        if current in votes:\n",
    "            votes[current] += 1  # dict can be assign like this*\n",
    "        elif current not in votes: \n",
    "            votes[current] = 1\n",
    "    # Note that sorted is a function that operates on iterators, so we convert the dictionary votes to a list via the items function\n",
    "    sortedVotes = sorted(votes.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    \n",
    "    return sortedVotes[0][0] # Take the key in the key-value pair, which consists of the majority vote class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Test your method on the following samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n"
     ]
    }
   ],
   "source": [
    "neighbors = [[1,1,1,'a'], [2,2,2,'a'], [3,3,3,'b'], [2.5,2.5,2.5,'b'], [3,3,4, 'a'], [2,2,2,'c'], [5,5,5, 'a']]\n",
    "response = predict(neighbors)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measuring Model Performance\n",
    "\n",
    "######  We're basically ready to test the performance of our very own k-NN implementation! One popular classification metric is accuracy, use the following method to check how well our k-NN algorithm performs on the test set we left aside."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAccuracy(testSet, predictions):\n",
    "    correct = 0\n",
    "    for x in range(len(testSet)):\n",
    "        if (testSet[x][-1] == predictions[x]):\n",
    "            correct += 1\n",
    "    return (correct/float(len(testSet))) * 100.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test the method on the following samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66.66666666666666\n"
     ]
    }
   ],
   "source": [
    "testSet = [[1,1,1,'a'], [2,2,2,'a'], [3,3,3,'b']]\n",
    "predictions = ['a', 'a', 'a']\n",
    "accuracy = getAccuracy(testSet, predictions)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######  Make predictions on the test set for different values of k. Compare the accuracy for different values and plot test performance, explaining why you think performance increases or decreases for different values of k (WRITE ANSWER IN BOX BELOW PLOT). Loop from k=1 to k=49 (inclusively), only considering cases where k is odd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 93.33333333333333, 3: 93.33333333333333, 5: 95.55555555555556, 7: 95.55555555555556, 9: 95.55555555555556, 11: 95.55555555555556, 13: 95.55555555555556, 15: 95.55555555555556, 17: 95.55555555555556, 19: 93.33333333333333, 21: 93.33333333333333, 23: 88.88888888888889, 25: 93.33333333333333, 27: 91.11111111111111, 29: 93.33333333333333, 31: 93.33333333333333, 33: 91.11111111111111, 35: 88.88888888888889, 37: 88.88888888888889, 39: 91.11111111111111, 41: 88.88888888888889, 43: 88.88888888888889, 45: 88.88888888888889, 47: 88.88888888888889, 49: 88.88888888888889}\n"
     ]
    }
   ],
   "source": [
    "### YOUR CODE HERE - Create a dictionary to store accuracies for different values of k.\n",
    "\n",
    "accuracies = {}\n",
    "\n",
    "for k in range(1, 50, 2):\n",
    "    predictions = []\n",
    "    \n",
    "    for x in iris_test:\n",
    "        \n",
    "        neighbours = getNeighbours(iris_train, x, k)\n",
    "        result = predict(neighbours)\n",
    "        predictions.append(result)\n",
    "\n",
    "    accuracy = getAccuracy(iris_test, predictions)\n",
    "    \n",
    "    accuracies[k] = accuracy\n",
    "\n",
    "print(accuracies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Plot a bar chart, having the different values of k on the x-axis and the test accuracy (in %) on the y-axis. Add a plot title, and choose a suitable format for readability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAM3klEQVR4nO3cXaxl5V3H8e9PpkRLjQzlZIK8OGhJDWmikBPE0DSkqKGtEUwIgZg6NpjxApRaE8He0BsTampfTAzJWNBpgrSEYocoUclIU71w0jOUlJexMkEoMxlmTkPpi15U5O/FWehxPGfmnL32njP8z/eTkL332mvv/Tws+M6aZ7+kqpAk9fJDGz0ASdL0GXdJasi4S1JDxl2SGjLuktSQcZekhk4a9yT3JTmW5Oll285J8liS54bLrcP2JPmTJAeTfD3J5bMcvCRpZWs5c/8L4Nrjtt0J7K2qS4C9w22A9wGXDP/sBO6ZzjAlSetx0rhX1VeAV47bfB2we7i+G7h+2fbP1ZJ/Bs5Oct60BitJWpstEz5uW1UdGa6/DGwbrp8PvLRsv0PDtiOcwLnnnlvbt2+fcCiStDnt37//W1U1t9J9k8b9f1RVJVn3bxgk2cnS0g0XXXQRCwsLY4ciSZtKkhdXu2/ST8scfWO5Zbg8Nmw/DFy4bL8Lhm3/T1Xtqqr5qpqfm1vxDx5J0oQmjfsjwI7h+g5gz7Ltvz58auZK4DvLlm8kSafISZdlkjwAXA2cm+QQcBdwN/BgkluAF4Ebh90fBd4PHAT+A/jQDMYsSTqJk8a9qm5e5a5rVti3gFvHDkqSNI7fUJWkhoy7JDVk3CWpIeMuSQ0Zd0lqaPQ3VDeD7Xf+zZr2e+HuD5yS/df6mPXuP2ZMk8xB0ux45i5JDRl3SWrIuEtSQ8Zdkhp607+heirejNRsnI5v2s76v43Tcc7qyTN3SWrIuEtSQ8Zdkhp606+5SxvpdFwTP93eUzod/x1tBp65S1JDxl2SGjLuktSQcZekhnxDVW35Rt7anI5frDodf8V01r/EOm2euUtSQ8Zdkhoy7pLUkHGXpIaMuyQ1ZNwlqSHjLkkNGXdJasi4S1JDxl2SGjLuktSQcZekhoy7JDVk3CWpIeMuSQ2NinuS303yTJKnkzyQ5IeTXJxkX5KDSb6Q5MxpDVaStDYTxz3J+cDvAPNV9S7gDOAm4OPAp6rqHcC3gVumMVBJ0tqNXZbZAvxIki3AW4EjwHuBh4b7dwPXj3wNSdI6TRz3qjoMfAL4JktR/w6wH3i1ql4bdjsEnL/S45PsTLKQZGFxcXHSYUiSVjBmWWYrcB1wMfDjwFnAtWt9fFXtqqr5qpqfm5ubdBiSpBWMWZb5BeDfqmqxqv4TeBi4Cjh7WKYBuAA4PHKMkqR1GhP3bwJXJnlrkgDXAM8CjwM3DPvsAPaMG6Ikab3GrLnvY+mN0yeAp4bn2gXcAXwkyUHg7cC9UxinJGkdtpx8l9VV1V3AXcdtfh64YszzSpLG8RuqktSQcZekhoy7JDVk3CWpIeMuSQ0Zd0lqyLhLUkPGXZIaMu6S1JBxl6SGjLskNWTcJakh4y5JDRl3SWrIuEtSQ8Zdkhoy7pLUkHGXpIaMuyQ1ZNwlqSHjLkkNGXdJasi4S1JDxl2SGjLuktSQcZekhoy7JDVk3CWpIeMuSQ0Zd0lqyLhLUkPGXZIaMu6S1JBxl6SGRsU9ydlJHkryL0kOJPn5JOckeSzJc8Pl1mkNVpK0NmPP3D8D/G1V/TTwM8AB4E5gb1VdAuwdbkuSTqGJ457kx4D3APcCVNUPqupV4Dpg97DbbuD6sYOUJK3PmDP3i4FF4M+TfC3JZ5OcBWyrqiPDPi8D28YOUpK0PmPivgW4HLinqi4D/p3jlmCqqoBa6cFJdiZZSLKwuLg4YhiSpOONifsh4FBV7RtuP8RS7I8mOQ9guDy20oOraldVzVfV/Nzc3IhhSJKON3Hcq+pl4KUk7xw2XQM8CzwC7Bi27QD2jBqhJGndtox8/G8D9yc5E3ge+BBLf2A8mOQW4EXgxpGvIUlap1Fxr6ongfkV7rpmzPNKksbxG6qS1JBxl6SGjLskNWTcJakh4y5JDRl3SWrIuEtSQ8Zdkhoy7pLUkHGXpIaMuyQ1ZNwlqSHjLkkNGXdJasi4S1JDxl2SGjLuktSQcZekhoy7JDVk3CWpIeMuSQ0Zd0lqyLhLUkPGXZIaMu6S1JBxl6SGjLskNWTcJakh4y5JDRl3SWrIuEtSQ8Zdkhoy7pLUkHGXpIZGxz3JGUm+luSvh9sXJ9mX5GCSLyQ5c/wwJUnrMY0z99uBA8tufxz4VFW9A/g2cMsUXkOStA6j4p7kAuADwGeH2wHeCzw07LIbuH7Ma0iS1m/smfungd8HXh9uvx14tapeG24fAs4f+RqSpHWaOO5Jfhk4VlX7J3z8ziQLSRYWFxcnHYYkaQVjztyvAn4lyQvA51lajvkMcHaSLcM+FwCHV3pwVe2qqvmqmp+bmxsxDEnS8SaOe1X9QVVdUFXbgZuAf6iqXwMeB24YdtsB7Bk9SknSuszic+53AB9JcpClNfh7Z/AakqQT2HLyXU6uqr4MfHm4/jxwxTSeV5I0Gb+hKkkNGXdJasi4S1JDxl2SGjLuktSQcZekhoy7JDVk3CWpIeMuSQ0Zd0lqyLhLUkPGXZIaMu6S1JBxl6SGjLskNWTcJakh4y5JDRl3SWrIuEtSQ8Zdkhoy7pLUkHGXpIaMuyQ1ZNwlqSHjLkkNGXdJasi4S1JDxl2SGjLuktSQcZekhoy7JDVk3CWpIeMuSQ0Zd0lqyLhLUkMTxz3JhUkeT/JskmeS3D5sPyfJY0meGy63Tm+4kqS1GHPm/hrwe1V1KXAlcGuSS4E7gb1VdQmwd7gtSTqFJo57VR2pqieG698DDgDnA9cBu4fddgPXjx2kJGl9prLmnmQ7cBmwD9hWVUeGu14Gtq3ymJ1JFpIsLC4uTmMYkqTB6LgneRvwReDDVfXd5fdVVQG10uOqaldVzVfV/Nzc3NhhSJKWGRX3JG9hKez3V9XDw+ajSc4b7j8PODZuiJKk9RrzaZkA9wIHquqTy+56BNgxXN8B7Jl8eJKkSWwZ8dirgA8CTyV5ctj2UeBu4MEktwAvAjeOG6Ikab0mjntV/ROQVe6+ZtLnlSSN5zdUJakh4y5JDRl3SWrIuEtSQ8Zdkhoy7pLUkHGXpIaMuyQ1ZNwlqSHjLkkNGXdJasi4S1JDxl2SGjLuktSQcZekhoy7JDVk3CWpIeMuSQ0Zd0lqyLhLUkPGXZIaMu6S1JBxl6SGjLskNWTcJakh4y5JDRl3SWrIuEtSQ8Zdkhoy7pLUkHGXpIaMuyQ1ZNwlqSHjLkkNzSTuSa5N8o0kB5PcOYvXkCStbupxT3IG8KfA+4BLgZuTXDrt15EkrW4WZ+5XAAer6vmq+gHweeC6GbyOJGkVs4j7+cBLy24fGrZJkk6RVNV0nzC5Abi2qn5zuP1B4Oeq6rbj9tsJ7BxuvhP4xhSHcS7wrSk+35vFZpy3c948NuO8Tzbnn6iquZXu2DKDwRwGLlx2+4Jh2/9RVbuAXTN4fZIsVNX8LJ77dLYZ5+2cN4/NOO8xc57FssxXgUuSXJzkTOAm4JEZvI4kaRVTP3OvqteS3Ab8HXAGcF9VPTPt15EkrW4WyzJU1aPAo7N47jWayXLPm8BmnLdz3jw247wnnvPU31CVJG08f35AkhpqF/fN+NMHSV5I8lSSJ5MsbPR4ZiXJfUmOJXl62bZzkjyW5LnhcutGjnHaVpnzx5IcHo73k0nev5FjnLYkFyZ5PMmzSZ5Jcvuwve2xPsGcJz7WrZZlhp8++FfgF1n68tRXgZur6tkNHdiMJXkBmK+q1p8BTvIe4PvA56rqXcO2PwJeqaq7hz/Mt1bVHRs5zmlaZc4fA75fVZ/YyLHNSpLzgPOq6okkPwrsB64HfoOmx/oEc76RCY91tzN3f/qgsar6CvDKcZuvA3YP13ez9D9EG6vMubWqOlJVTwzXvwccYOlb7m2P9QnmPLFucd+sP31QwN8n2T9883cz2VZVR4brLwPbNnIwp9BtSb4+LNu0WZ44XpLtwGXAPjbJsT5uzjDhse4W983q3VV1OUu/xHnr8Ff5TaeW1hj7rDOu7h7gp4CfBY4Af7yxw5mNJG8Dvgh8uKq+u/y+rsd6hTlPfKy7xX1NP33QTVUdHi6PAX/F0vLUZnF0WK98Y93y2AaPZ+aq6mhV/VdVvQ78GQ2Pd5K3sBS5+6vq4WFz62O90pzHHOtucd90P32Q5KzhDRiSnAX8EvD0iR/VyiPAjuH6DmDPBo7llHgjcINfpdnxThLgXuBAVX1y2V1tj/Vqcx5zrFt9WgZg+KjQp/nfnz74ww0e0kwl+UmWztZh6RvHf9l1zkkeAK5m6ZfyjgJ3AV8CHgQuAl4EbqyqNm9ArjLnq1n6a3oBLwC/tWwt+k0vybuBfwSeAl4fNn+UpTXolsf6BHO+mQmPdbu4S5L6LctIkjDuktSScZekhoy7JDVk3CWpIeMuSQ0Zd0lqyLhLUkP/DZpjhsacZ4J2AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "### YOUR CODE HERE - Create a bar plot\n",
    "\n",
    "#plt.figure(figsize=(10, 6))\n",
    "#plt.title('Values of k in kNN Implementation versus Test Accuracy in %')\n",
    "plt.bar(range(len(accuracies)), accuracies.values())\n",
    "#plt.xticks(range(len(accuracies)), list(accuracies.keys()))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"background-color: #F9F2EB\"> YOUR EXPLANATION HERE...  </span>\n",
    "*(Hint: think of the bias-variance trade-off/ AND underfitting/overfitting)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Classification Algorithm Comparison\n",
    "\n",
    "Now that we've learned about a couple of classification algorithms, you must be wondering: which algorithm should I choose for a given task? In the following question, we'll learn to investigate model performance for various algorithms and then based off the best algorithm, do some hyperparameter tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 1) Import necessary packages from sklearn.\n",
    "\n",
    "Here are a few popular classification algorithms available from the sklearn API. If you're curious on what supervised learning algorithms are available, don't hesitate to read up on documentation: https://scikit-learn.org/stable/supervised_learning.html (this might be useful brainstorming for your hackathon idea!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 2) Load the Pima Indians Diabetes Dataset using pandas.\n",
    "\n",
    "Just another way of downloading a CSV file from online! Let's inspect the dataset we're working with, the Pima Indians Diabetes Dataset.\n",
    "\n",
    "\" This dataset is originally from the National Institute of Diabetes and Digestive and Kidney Diseases. The objective of the dataset is to diagnostically predict whether or not a patient has diabetes, based on certain diagnostic measurements included in the dataset. Several constraints were placed on the selection of these instances from a larger database. In particular, all patients here are females at least 21 years old of Pima Indian heritage. \" - Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>preg</th>\n",
       "      <th>plas</th>\n",
       "      <th>pres</th>\n",
       "      <th>skin</th>\n",
       "      <th>test</th>\n",
       "      <th>mass</th>\n",
       "      <th>pedi</th>\n",
       "      <th>age</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   preg  plas  pres  skin  test  mass   pedi  age  class\n",
       "0     6   148    72    35     0  33.6  0.627   50      1\n",
       "1     1    85    66    29     0  26.6  0.351   31      0\n",
       "2     8   183    64     0     0  23.3  0.672   32      1\n",
       "3     1    89    66    23    94  28.1  0.167   21      0\n",
       "4     0   137    40    35   168  43.1  2.288   33      1"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load dataset\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "\n",
    "df = pd.read_csv(url, names=names)  # names parameter is simply used to name the columns, defaults to None if unspecified\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 3) Take the values of the DataFrame as a numpy array, then separate the features and target variable columns into X, y respectively.\n",
    "\n",
    "We don't directly split up the train/test splits, because we'll be using the sklearn k cross-fold validation implementation, which takes in the entire dataset as an argument.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(768, 9)\n"
     ]
    }
   ],
   "source": [
    "array = df.values\n",
    "print(array.shape)\n",
    "\n",
    "X = array[:,0:8]  # Take first 8 columns as features\n",
    "y = array[:,8]  # Take last column as the target variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we're going to fix the random seed in the data partitioning step of k cross-fold validation, for the simple reason of wanting to minimize performance differences to be due to different data partitions seen by each algorithm. Hence, adding this seed will cause every algorithm to see the same shuffled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix random seed in data partitions\n",
    "seed = 7\n",
    "\n",
    "# Prepare models\n",
    "models = []\n",
    "models.append(('LR', LogisticRegression(solver='lbfgs', max_iter=500)))\n",
    "models.append(('LDA', LinearDiscriminantAnalysis()))\n",
    "models.append(('KNN', KNeighborsClassifier()))\n",
    "models.append(('CART', DecisionTreeClassifier()))\n",
    "models.append(('RF', RandomForestClassifier(n_estimators=100)))\n",
    "models.append(('NB', GaussianNB()))\n",
    "models.append(('SVM', SVC(gamma='auto')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 4) Now, we evaluate each model individually and store cross-validation results for preliminary comparison. Note that we haven't done any hyperparameter tuning as we have for our k-NN implementation previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR: 0.7760423786739576 (0.051575452620868226)\n",
      "LDA: 0.773462064251538 (0.05159180390446138)\n",
      "KNN: 0.7265550239234451 (0.06182131406705549)\n",
      "CART: 0.6887047163362953 (0.06426144694972535)\n",
      "RF: 0.768198906356801 (0.0653267877551359)\n",
      "NB: 0.7551777170198223 (0.04276593954064409)\n",
      "SVM: 0.6510252904989747 (0.07214083485055327)\n"
     ]
    }
   ],
   "source": [
    "# Evaluate each model individually\n",
    "\n",
    "results = []\n",
    "names = []\n",
    "scoring = 'accuracy'\n",
    "\n",
    "for name, model in models:\n",
    "    \n",
    "    kfold = model_selection.KFold(n_splits=10, random_state=seed)  # Provides train/test indices to split data in train/test sets.\n",
    "    cv_results = model_selection.cross_val_score(model, X, y, cv=kfold, scoring=scoring)  \n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    msg = \"{}: {} ({})\".format(name, cv_results.mean(), cv_results.std())  # Provides the mean accuracy, std dev for results\n",
    "    print(msg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 5) Use a boxplot to show results\n",
    "\n",
    "In both academia/research and industry, graphs are necessary to simply display results and convey messages more clearly. Below, we show a boxplot, where each horizontal line represents the median result obtained, the T-shaped ends show the highest and lowest results. The box simply contains where \"most\" the results lie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# boxplot algorithm comparison\n",
    "fig = plt.figure()\n",
    "fig.suptitle('Algorithm Comparison')\n",
    "ax = fig.add_subplot(111)\n",
    "plt.boxplot(results)\n",
    "ax.set_xticklabels(names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, it's your turn!\n",
    "\n",
    "After doing some preliminary comparison between a few classification algorithms, you can now select a few and do some hyperparameter tuning. For example, random forests did fairly well, and we haven't even tuned any of its hyperparameters (e.g. max tree depth, number of trees, etc.) To find optimal hyperparameters, it's common to perform a **grid search** over a parameter space. Essentially, you want to specify a range of values for different hyperparameters within an algorithm, and try out every combination of parameters that are evaluated on a test set left aside (Do you see the recurring theme of leaving data aside?). Typically, you'd select the hyperparameter values that worked best on your test set. \n",
    "\n",
    "Sklearn has an API for just that, which takes in a **model**, and a **dictionary of parameters** to search over: [scikit-learn docs](https://scikit-learn.org/stable/modules/grid_search.html#grid-search) . Follow the example [here](https://scikit-learn.org/stable/auto_examples/model_selection/plot_grid_search_digits.html#sphx-glr-auto-examples-model-selection-plot-grid-search-digits-py) to see how to use the API:\n",
    "- Select an algorithm of your choice from the sklearn classification models (Random Forests is a nice one though)\n",
    "- Used the **GridSearchCV** module to find optimal hyperparameters for your model (pick 2-3 parameters to search over!)\n",
    "- Let us know if you have any cool results to show!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning hyper-parameters for precision\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'min_samples_leaf': 2, 'n_estimators': 600}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "0.646 (+/-0.167) for {'min_samples_leaf': 1, 'n_estimators': 10}\n",
      "0.668 (+/-0.143) for {'min_samples_leaf': 1, 'n_estimators': 100}\n",
      "0.704 (+/-0.163) for {'min_samples_leaf': 1, 'n_estimators': 200}\n",
      "0.668 (+/-0.168) for {'min_samples_leaf': 1, 'n_estimators': 300}\n",
      "0.678 (+/-0.155) for {'min_samples_leaf': 1, 'n_estimators': 400}\n",
      "0.693 (+/-0.192) for {'min_samples_leaf': 1, 'n_estimators': 500}\n",
      "0.679 (+/-0.128) for {'min_samples_leaf': 1, 'n_estimators': 600}\n",
      "0.673 (+/-0.151) for {'min_samples_leaf': 1, 'n_estimators': 750}\n",
      "0.681 (+/-0.182) for {'min_samples_leaf': 1, 'n_estimators': 1000}\n",
      "0.637 (+/-0.115) for {'min_samples_leaf': 2, 'n_estimators': 10}\n",
      "0.695 (+/-0.203) for {'min_samples_leaf': 2, 'n_estimators': 100}\n",
      "0.697 (+/-0.155) for {'min_samples_leaf': 2, 'n_estimators': 200}\n",
      "0.672 (+/-0.134) for {'min_samples_leaf': 2, 'n_estimators': 300}\n",
      "0.700 (+/-0.205) for {'min_samples_leaf': 2, 'n_estimators': 400}\n",
      "0.694 (+/-0.213) for {'min_samples_leaf': 2, 'n_estimators': 500}\n",
      "0.709 (+/-0.183) for {'min_samples_leaf': 2, 'n_estimators': 600}\n",
      "0.697 (+/-0.175) for {'min_samples_leaf': 2, 'n_estimators': 750}\n",
      "0.694 (+/-0.173) for {'min_samples_leaf': 2, 'n_estimators': 1000}\n",
      "0.650 (+/-0.089) for {'min_samples_leaf': 3, 'n_estimators': 10}\n",
      "0.684 (+/-0.157) for {'min_samples_leaf': 3, 'n_estimators': 100}\n",
      "0.675 (+/-0.130) for {'min_samples_leaf': 3, 'n_estimators': 200}\n",
      "0.683 (+/-0.167) for {'min_samples_leaf': 3, 'n_estimators': 300}\n",
      "0.699 (+/-0.174) for {'min_samples_leaf': 3, 'n_estimators': 400}\n",
      "0.699 (+/-0.187) for {'min_samples_leaf': 3, 'n_estimators': 500}\n",
      "0.687 (+/-0.175) for {'min_samples_leaf': 3, 'n_estimators': 600}\n",
      "0.677 (+/-0.186) for {'min_samples_leaf': 3, 'n_estimators': 750}\n",
      "0.684 (+/-0.176) for {'min_samples_leaf': 3, 'n_estimators': 1000}\n",
      "\n",
      "Detailed classification report:\n",
      "\n",
      "The model is trained on the full development set.\n",
      "The scores are computed on the full evaluation set.\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.78      0.88      0.83       253\n",
      "         1.0       0.69      0.53      0.60       131\n",
      "\n",
      "   micro avg       0.76      0.76      0.76       384\n",
      "   macro avg       0.74      0.70      0.71       384\n",
      "weighted avg       0.75      0.76      0.75       384\n",
      "\n",
      "\n",
      "Tuning hyper-parameters for recall\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'min_samples_leaf': 3, 'n_estimators': 10}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "0.516 (+/-0.223) for {'min_samples_leaf': 1, 'n_estimators': 10}\n",
      "0.553 (+/-0.149) for {'min_samples_leaf': 1, 'n_estimators': 100}\n",
      "0.554 (+/-0.139) for {'min_samples_leaf': 1, 'n_estimators': 200}\n",
      "0.561 (+/-0.137) for {'min_samples_leaf': 1, 'n_estimators': 300}\n",
      "0.554 (+/-0.130) for {'min_samples_leaf': 1, 'n_estimators': 400}\n",
      "0.561 (+/-0.144) for {'min_samples_leaf': 1, 'n_estimators': 500}\n",
      "0.576 (+/-0.113) for {'min_samples_leaf': 1, 'n_estimators': 600}\n",
      "0.568 (+/-0.169) for {'min_samples_leaf': 1, 'n_estimators': 750}\n",
      "0.554 (+/-0.139) for {'min_samples_leaf': 1, 'n_estimators': 1000}\n",
      "0.567 (+/-0.226) for {'min_samples_leaf': 2, 'n_estimators': 10}\n",
      "0.553 (+/-0.155) for {'min_samples_leaf': 2, 'n_estimators': 100}\n",
      "0.576 (+/-0.167) for {'min_samples_leaf': 2, 'n_estimators': 200}\n",
      "0.561 (+/-0.134) for {'min_samples_leaf': 2, 'n_estimators': 300}\n",
      "0.524 (+/-0.153) for {'min_samples_leaf': 2, 'n_estimators': 400}\n",
      "0.539 (+/-0.162) for {'min_samples_leaf': 2, 'n_estimators': 500}\n",
      "0.546 (+/-0.170) for {'min_samples_leaf': 2, 'n_estimators': 600}\n",
      "0.539 (+/-0.170) for {'min_samples_leaf': 2, 'n_estimators': 750}\n",
      "0.539 (+/-0.132) for {'min_samples_leaf': 2, 'n_estimators': 1000}\n",
      "0.590 (+/-0.182) for {'min_samples_leaf': 3, 'n_estimators': 10}\n",
      "0.525 (+/-0.143) for {'min_samples_leaf': 3, 'n_estimators': 100}\n",
      "0.532 (+/-0.155) for {'min_samples_leaf': 3, 'n_estimators': 200}\n",
      "0.531 (+/-0.183) for {'min_samples_leaf': 3, 'n_estimators': 300}\n",
      "0.546 (+/-0.149) for {'min_samples_leaf': 3, 'n_estimators': 400}\n",
      "0.532 (+/-0.155) for {'min_samples_leaf': 3, 'n_estimators': 500}\n",
      "0.546 (+/-0.176) for {'min_samples_leaf': 3, 'n_estimators': 600}\n",
      "0.539 (+/-0.170) for {'min_samples_leaf': 3, 'n_estimators': 750}\n",
      "0.546 (+/-0.163) for {'min_samples_leaf': 3, 'n_estimators': 1000}\n",
      "\n",
      "Detailed classification report:\n",
      "\n",
      "The model is trained on the full development set.\n",
      "The scores are computed on the full evaluation set.\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.78      0.85      0.81       253\n",
      "         1.0       0.64      0.53      0.58       131\n",
      "\n",
      "   micro avg       0.74      0.74      0.74       384\n",
      "   macro avg       0.71      0.69      0.70       384\n",
      "weighted avg       0.73      0.74      0.73       384\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### YOUR CODE HERE - Find optimal hyperparameters for an algorithm of your choice\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Split the dataset in two equal parts\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.5, random_state=0)\n",
    "\n",
    "# Set the parameters by cross-validation\n",
    "tuned_parameters = [{'n_estimators': [10, 100, 200, 300, 400, 500, 600, 750, 1000], 'min_samples_leaf': [1,2,3]}]\n",
    "\n",
    "scores = ['precision', 'recall']\n",
    "\n",
    "for score in scores:\n",
    "    print(\"Tuning hyper-parameters for {}\".format(score))\n",
    "    print()\n",
    "\n",
    "    clf = GridSearchCV(RandomForestClassifier(), tuned_parameters, cv=5,\n",
    "                       scoring=score, iid=False)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    print(\"Best parameters set found on development set:\")\n",
    "    print()\n",
    "    print(clf.best_params_)\n",
    "    print()\n",
    "    print(\"Grid scores on development set:\")\n",
    "    print()\n",
    "    means = clf.cv_results_['mean_test_score']\n",
    "    stds = clf.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "              % (mean, std * 2, params))\n",
    "    print()\n",
    "\n",
    "    print(\"Detailed classification report:\")\n",
    "    print()\n",
    "    print(\"The model is trained on the full development set.\")\n",
    "    print(\"The scores are computed on the full evaluation set.\")\n",
    "    print()\n",
    "    y_true, y_pred = y_test, clf.predict(X_test)\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Now you're familiar with fundamental, but VERY important algorithm selection and evaluation techniques! Congrats :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
